# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# This file contains Prometheus alerting rules for {{ .Values.namespace }} namespace.
# The alert severity labels determine which alertmanager receiver the alerts are sent to.
# ‼️ Critical alert - send to {{ .Values.serviceName }}
# ⚠️ Warning alert - send to {{ .Values.serviceName }}
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  namespace: "{{ .Values.namespace }}"
  labels:
    role: alert-rules
  name: "alerting-rules-{{ .Values.serviceName }}"
spec:
  groups:
    - name: kubernetes-apps
      rules:
        - alert: KubeQuotaAlmostFull # ‼️ Critical alert
          annotations:
            description: Namespace {{ .Values.namespace }} is using {{ "{{" }} $value | humanizePercentage {{ "}}" }} of its {{ "{{" }} $labels.resource {{ "}}" }} quota.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaalmostfull
            summary: Namespace quota is going to be full.
          expr: |
            kube_resourcequota{job="kube-state-metrics", type="used", namespace="{{ .Values.namespace }}"}
              / ignoring(instance, job, type)
            (kube_resourcequota{job="kube-state-metrics", type="hard", namespace="{{ .Values.namespace }}"} > 0)
              > 0.9 < 1
          for: 15m
          labels:
            severity: {{ .Values.alertLabels.severity }}
            job: {{ .Values.serviceName }}
        - alert: KubeQuota-Exceeded # ‼️ Critical alert
          annotations:
            message: Namespace {{ .Values.namespace }} is using {{ "{{" }} printf "%0.0f" $value
              {{ "}}" }}% of its {{ "{{" }} $labels.resource {{ "}}" }} quota.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded
          expr: |-
            100 * kube_resourcequota{job="kube-state-metrics", type="used", namespace="{{ .Values.namespace }}"} 
            / ignoring(instance, job, type)
            (kube_resourcequota{job="kube-state-metrics", type="hard", namespace="{{ .Values.namespace }}"} > 0)
            > 95
          for: 15m
          labels:
            severity: {{ .Values.alertLabels.severity }}
            job: {{ .Values.serviceName }}
        - alert: KubePodCrashLooping # ‼️ Critical alert
          # pint file/disable alerts/template
          annotations:
            message: Pod {{ .Values.namespace }}/{{ "{{" }} $labels.pod {{ "}}" }} ({{ "{{" }} $labels.container
              {{ "}}" }}) is restarting {{ "{{" }} printf "%.2f" $value {{ "}}" }} times per minute.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping
          expr: |-
            rate(kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace="{{ .Values.namespace }}"}[5m]) * 60 > 1
          for: 5m
          labels:
            severity: {{ .Values.alertLabels.severity }}
            job: {{ .Values.serviceName }}
        - alert: KubePodNotReady
          annotations:
            message: A single pod {{ .Values.namespace }}/{{ "{{" }} $labels.pod {{ "}}" }} has been in a non-ready
              state for longer than 5 minutes.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
          expr: |-
            sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown", namespace="{{ .Values.namespace }}"}) 
            > 0
          for: 5m
          labels:
            severity: {{ .Values.alertLabels.severity }}
            job: {{ .Values.serviceName }}
        - alert: KubePodsMultipleNotReady # ‼️ Critical alert
          annotations:
            message: 2 or more pods in {{ .Values.namespace }} namespace have been in a non-ready
              state for longer than 5 minutes.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
          expr: |-
            sum by (namespace) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown", namespace="{{ .Values.namespace }}"}) 
            >= 2
          for: 5m
          labels:
            severity: {{ .Values.alertLabels.severity }}
            job: {{ .Values.serviceName }}
        - alert: KubeDeploymentGenerationMismatch # ‼️ Critical alert
          annotations:
            message: Deployment generation for {{ .Values.namespace }}/{{ "{{" }} $labels.deployment {{ "}}" }} does not match, this indicates that the Deployment has failed but has not been rolled back.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch
          expr: |-
            kube_deployment_status_observed_generation{job="kube-state-metrics", namespace="{{ .Values.namespace }}"}
            !=
            kube_deployment_metadata_generation{job="kube-state-metrics", namespace="{{ .Values.namespace }}"}
          for: 15m
          labels:
            severity: {{ .Values.alertLabels.severity }}
            job: {{ .Values.serviceName }}
        - alert: TrackMyCaseServiceDown
          annotations:
            message: >-
              {{ .Values.serviceName }} service is completely DOWN in {{ .Values.namespace }} namespace
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeservicedown
          expr: |-
            absent(up{namespace="{{ .Values.namespace }}", service="{{ .Values.serviceName }}"} == 1)
          for: 2m
          labels:
            severity: {{ .Values.alertLabels.severity }}
            job: {{ .Values.serviceName }}
        - alert: HighPodMemoryUsage
          annotations:
            message: >-
              Pod {{ "{{" }} $labels.pod {{ "}}" }} memory usage is greater than 90%
          expr: |-
            (sum by (namespace, pod) (container_memory_working_set_bytes{namespace="{{ .Values.namespace }}", pod=~"{{ .Values.serviceName }}-.*", container!=""}) 
            / sum by (namespace, pod) (container_spec_memory_limit_bytes{namespace="{{ .Values.namespace }}", pod=~"{{ .Values.serviceName }}-.*", container!=""})) 
            > 0.9
          for: 5m
          labels:
            severity: {{ .Values.alertLabels.severity }}
            job: {{ .Values.serviceName }}
